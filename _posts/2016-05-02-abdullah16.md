---
title: Sketching, Embedding and Dimensionality Reduction in Information Theoretic
  Spaces
abstract: In this paper we show how to embed information distances like the χ^2 and
  Jensen-Shannon divergences efficiently in low dimensional spaces while preserving
  all  pairwise distances.  We then prove a dimensionality reduction result for the
  Hellinger, Jensen–Shannon, and χ^2 divergences that preserves the information geometry
  of the distributions, specifically, by retaining the simplex structure of the space.
  While our first result already implies these divergences can be explicitly embedded
  in the Euclidean space, retaining the simplex structure is important because it
  allows us to do inferences in the reduced space.  We also show that these divergences  can
  be sketched efficiently (i.e., up to a multiplicative error in sublinear space)
  in the aggregate streaming model. This result is exponentially stronger than known
  upper bounds for sketching these distances in the strict turnstile streaming model.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: abdullah16
month: 0
tex_title: Sketching, Embedding and Dimensionality Reduction in Information Theoretic
  Spaces
firstpage: 948
lastpage: 956
page: 948-956
order: 948
cycles: false
author:
- given: Amirali
  family: Abdullah
- given: Ravi
  family: Kumar
- given: Andrew
  family: McGregor
- given: Sergei
  family: Vassilvitskii
- given: Suresh
  family: Venkatasubramanian
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/abdullah16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
