---
title: Early Stopping as Nonparametric Variational Inference
abstract: We show that unconverged stochastic gradient descent can be interpreted
  as sampling from a nonparametric approximate posterior distribution. This distribution
  is implicitly defined by the transformation of an initial distribution by a sequence
  of optimization steps.  By tracking the change in entropy of this distribution during
  optimization, we give a scalable, unbiased estimate of a variational lower bound
  on the log marginal likelihood. This bound can be used to optimize hyperparameters
  instead of cross-validation. This Bayesian interpretation of SGD also suggests new
  overfitting-resistant optimization procedures, and gives a theoretical foundation
  for early stopping and ensembling. We investigate the properties of this marginal
  likelihood estimator on neural network models.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: duvenaud16
month: 0
tex_title: Early Stopping as Nonparametric Variational Inference
firstpage: 1070
lastpage: 1077
page: 1070-1077
sections: 
author:
- given: David
  family: Duvenaud
- given: Dougal
  family: Maclaurin
- given: Ryan
  family: Adams
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/duvenaud16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
