---
supplementary: Supplementary:ghashami16-supp.pdf
title: Streaming Kernel Principal Component Analysis
abstract: Kernel principal component analysis (KPCA) provides a concise set of basis
  vectors which capture non-linear structures within large data sets, and is a central
  tool in data analysis and learning.  To allow for non-linear relations, typically
  a full n x n kernel matrix is constructed over n data points, but this requires
  too much space and time for large values of n.  Techniques such as the Nystrom method
  and random feature maps can help towards this goal, but they do not explicitly maintain
  the basis vectors in a stream and take more space than desired.  We propose a new
  approach for streaming KPCA which maintains a small set of basis elements in a stream,
  requiring space only logarithmic in n, and also improves the dependence on the error
  parameter.  Our technique combines together random feature maps with recent advances
  in matrix sketching, it has guaranteed spectral norm error bounds with respect to
  the original kernel matrix, and it compares favorably in practice to state-of-the-art
  approaches.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: ghashami16
month: 0
tex_title: Streaming Kernel Principal Component Analysis
firstpage: 1365
lastpage: 1374
page: 1365-1374
sections: 
author:
- given: Mina
  family: Ghashami
- given: Daniel J.
  family: Perry
- given: Jeff
  family: Phillips
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/ghashami16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
