---
supplementary: http://proceedings.mlr.press/v51/goldstein16-supp.pdf
title: 'Unwrapping ADMM: Efficient Distributed Computing via Transpose Reduction'
abstract: Recent approaches to distributed model fitting rely heavily on consensus
  ADMM, where each node solves small sub-problems using only local data.  We propose
  iterative methods that solve global sub-problems over an entire distributed dataset.  This
  is possible using transpose reduction strategies that allow a single node to solve
  least-squares over massive datasets without putting all the data in one place.  This
  results in simple iterative methods that avoid the expensive inner loops required
  for consensus methods.  We analyze the convergence rates of the proposed schemes
  and demonstrate the efficiency of this approach by fitting linear classifiers and
  sparse linear models to large datasets using a distributed implementation with up
  to 20,000 cores in far less time than previous approaches.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: goldstein16
month: 0
tex_title: 'Unwrapping ADMM: Efficient Distributed Computing via Transpose Reduction'
firstpage: 1151
lastpage: 1158
page: 1151-1158
order: 1151
cycles: false
author:
- given: Tom
  family: Goldstein
- given: Gavin
  family: Taylor
- given: Kawika
  family: Barabin
- given: Kent
  family: Sayre
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/goldstein16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
