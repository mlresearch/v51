---
title: Nearly Optimal Classification for Semimetrics
abstract: We initiate the rigorous study of classification in semimetric spaces, which
  are point sets with a distance function that is non-negative and symmetric, but
  need not satisfy the triangle inequality. We define the \em density dimension \dens
  and discover that it plays a central role in the statistical and algorithmic feasibility
  of learning in semimetric spaces. We compute this quantity for several widely used
  semimetrics and present nearly optimal sample compression algorithms, which are
  then used to obtain generalization guarantees, including fast rates.  Our claim
  of near-optimality holds in both computational and statistical senses. When the
  sample has radius R and margin γ, we show that it can be compressed down to roughly
  d=(R/γ)^\dens points, and further that finding a significantly better compression
  is algorithmically intractable unless P=NP. This compression implies generalization
  via standard Occam-type arguments, to which we provide a nearly matching lower bound.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: gottlieb16
month: 0
tex_title: Nearly Optimal Classification for Semimetrics
firstpage: 379
lastpage: 388
page: 379-388
order: 379
cycles: false
author:
- given: Lee-Ad
  family: Gottlieb
- given: Aryeh
  family: Kontorovich
- given: Pinhas
  family: Nisnevitch
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/gottlieb16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
