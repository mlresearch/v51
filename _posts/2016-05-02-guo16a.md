---
title: Quantization based Fast Inner Product Search
abstract: We propose a quantization based approach for fast approximate Maximum Inner
  Product Search (MIPS). Each database vector is quantized in multiple subspaces via
  a set of codebooks, learned directly by minimizing the inner product quantization
  error. Then, the inner product of a query to a database vector is approximated as
  the sum of inner products with the subspace quantizers. Different from recently
  proposed LSH approaches to MIPS, the database vectors and queries do not need to
  be augmented in a higher dimensional feature space. We also provide a theoretical
  analysis of the proposed approach, consisting of the concentration results under
  mild assumptions. Furthermore, if a small set of held-out queries is given at the
  training time, we propose a modified codebook learning procedure which further improves
  the accuracy.  Experimental results on a variety of datasets including those arising
  from deep neural networks show that the proposed approach significantly outperforms
  the existing state-of-the-art.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: guo16a
month: 0
firstpage: 482
lastpage: 490
page: 482-490
sections: 
author:
- given: Ruiqi
  family: Guo
- given: Sanjiv
  family: Kumar
- given: Krzysztof
  family: Choromanski
- given: David
  family: Simcha
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/guo16a.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
