---
title: Improper Deep Kernels
abstract: Neural networks have recently re-emerged as a powerful hypothesis class,
  yielding impressive classification accuracy in multiple domains. However, their
  training is a non convex optimization problem. Here we address this difficulty by
  turning to "improper learning" of neural nets. In other words, we learn a classifier
  that is not a neural net but is competitive with the best neural net model given
  a sufficient number of training examples. Our approach relies on a novel kernel
  which integrates over the set of possible neural models. It turns out that the corresponding
  integral can be evaluated in closed form via a simple recursion. The learning problem
  is then an SVM with this kernel, and a global optimum can thus be found efficiently.
  We also provide sample complexity results which depend on the stability of the optimal
  neural net.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: heinemann16
month: 0
tex_title: Improper Deep Kernels
firstpage: 1159
lastpage: 1167
page: 1159-1167
sections: 
author:
- given: Uri
  family: Heinemann
- given: Roi
  family: Livni
- given: Elad
  family: Eban
- given: Gal
  family: Elidan
- given: Amir
  family: Globerson
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/heinemann16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
