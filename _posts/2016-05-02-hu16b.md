---
title: "(Bandit) Convex Optimization with Biased Noisy Gradient Oracles"
abstract: A popular class of algorithms for convex optimization and online learning
  with bandit feedback rely on constructing noisy gradient estimates, which are then
  used in place of the actual gradients in appropriately adjusted first-order algorithms.
  Depending on the properties of the function to be optimized and the nature of “noise”
  in the bandit feedback, the bias and variance of gradient estimates exhibit various
  tradeoffs. In this paper we propose a novel framework that replaces the specific
  gradient estimation methods with an abstract oracle model. With the help of the
  new framework we unify previous works, reproducing their results in a clean and
  concise fashion, while, perhaps more importantly, the framework also allows us to
  formally show that to achieve the optimal root-n rate either the algorithms that
  use existing gradient estimators, or the proofs have to go beyond what exists today.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hu16b
month: 0
tex_title: "(Bandit) Convex Optimization with Biased Noisy Gradient Oracles"
firstpage: 819
lastpage: 828
page: 819-828
sections: 
author:
- given: Xiaowei
  family: Hu
- given: Prashanth
  family: L.A.
- given: András
  family: György
- given: Csaba
  family: Szepesvari
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/hu16b.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
