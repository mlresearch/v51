---
supplementary: Supplementary:kaganovsky16-supp.pdf
title: Parallel Majorization Minimization with Dynamically Restricted Domains for
  Nonconvex Optimization
abstract: We propose an optimization framework for nonconvex problems based on majorization-minimization
  that is particularity well-suited for parallel computing. It reduces the optimization
  of a high dimensional nonconvex objective function to successive optimizations of
  locally tight and convex upper bounds which are additively separable into low dimensional
  objectives. The original problem is then broken into simpler and parallel tasks,
  while guaranteeing the monotonic reduction of the original objective function and
  convergence to a local minimum. This framework also allows one to restrict the upper
  bound to a local dynamic convex domain, so that the bound is better matched to the
  local curvature of the objective function, resulting in accelerated convergence.
  We test the proposed framework on a nonconvex support vector machine based on a
  sigmoid loss function and on nonconvex penalized logistic regression.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: kaganovsky16
month: 0
firstpage: 1497
lastpage: 1505
page: 1497-1505
sections: 
author:
- given: Yan
  family: Kaganovsky
- given: Ikenna
  family: Odinaka
- given: David
  family: Carlson
- given: Lawrence
  family: Carin
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/kaganovsky16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
