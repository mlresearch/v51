---
supplementary: Supplementary:le16-supp.pdf
title: Nonparametric Budgeted Stochastic Gradient Descent
abstract: One of the most challenging problems in kernel online learning is to bound
  the model size. Budgeted kernel online learning addresses this issue by bounding
  the model size to a predefined budget. However, determining an appropriate value
  for such predefined budget is arduous. In this paper, we propose the Nonparametric
  Budgeted Stochastic Gradient Descent that allows the model size to automatically
  grow with data in a principled way. We provide theoretical analysis to show that
  our framework is guaranteed to converge for a large collection of loss functions
  (e.g. Hinge, Logistic, L2, L1, and \varepsilon-insensitive) which enables the proposed
  algorithm to perform both classification and regression tasks without hurting the
  ideal convergence rate O\left(\frac1T\right) of the standard Stochastic Gradient
  Descent. We validate our algorithm on the real-world datasets to consolidate the
  theoretical claims.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: le16
month: 0
tex_title: Nonparametric Budgeted Stochastic Gradient Descent
firstpage: 654
lastpage: 572
page: 654-572
sections: 
author:
- given: Trung
  family: Le
- given: Vu
  family: Nguyen
- given: Tu Dinh
  family: Nguyen
- given: Dinh
  family: Phung
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/le16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
