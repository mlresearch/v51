---
title: On Sparse Variational Methods and the Kullback-Leibler Divergence between Stochastic
  Processes
abstract: The variational framework for learning inducing variables (Titsias, 2009)
  has had a large impact on the Gaussian process literature. The framework may be
  interpreted as minimizing a rigorously defined Kullback-Leibler divergence between
  the approximating and posterior processes. To our knowledge this connection has
  thus far gone unremarked in the literature. In this paper we give a substantial
  generalization of the literature on this topic. We give a new proof of the result
  for infinite index sets which allows inducing points that are not data points and
  likelihoods that depend on all function values. We then discuss augmented index
  sets and show that, contrary to previous works, marginal consistency of augmentation
  is not enough to guarantee consistency of variational inference with the original
  model. We then characterize an extra condition where such a guarantee is obtainable.
  Finally we show how our framework sheds light on interdomain sparse approximations
  and sparse approximations for Cox processes.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: matthews16
month: 0
tex_title: On Sparse Variational Methods and the Kullback-Leibler Divergence between
  Stochastic Processes
firstpage: 231
lastpage: 239
page: 231-239
order: 231
cycles: false
author:
- given: Alexander G. de G.
  family: Matthews
- given: James
  family: Hensman
- given: Richard
  family: Turner
- given: Zoubin
  family: Ghahramani
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/matthews16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
