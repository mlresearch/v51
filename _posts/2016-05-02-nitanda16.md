---
supplementary: Supplementary:nitanda16-supp.pdf
title: Accelerated Stochastic Gradient Descent for Minimizing Finite Sums
abstract: We propose an optimization method for minimizing the finite sums of smooth
  convex functions. Our method incorporates an accelerated gradient descent (AGD)
  and a stochastic variance reduction gradient (SVRG) in a mini-batch setting. An
  important feature of the method is that it can be directly applied to general convex
  and semi-strongly convex problems that is a weaker condition than strong convexity.
  We show that our method achieves a better overall complexity for the general convex
  problems and linear convergence for optimal strongly convex problems. Moreover we
  prove the fast iteration complexity of our method. Our experiments show the effectiveness
  of our method.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: nitanda16
month: 0
firstpage: 195
lastpage: 203
page: 195-203
sections: 
author:
- given: Atsushi
  family: Nitanda
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/nitanda16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
