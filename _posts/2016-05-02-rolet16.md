---
supplementary: Supplementary:rolet16-supp.pdf
title: Fast Dictionary Learning with a Smoothed Wasserstein Loss
abstract: We consider in this paper the dictionary learning problem when the observations
  are normalized histograms of features. This problem can be tackled using non-negative
  matrix factorization approaches, using typically Euclidean or Kullback-Leibler fitting
  errors. Because these fitting errors are separable and treat each feature on equal
  footing, they are blind to any similarity the features may share. We assume in this
  work that we have prior knowledge on these features. To leverage this side-information,
  we propose to use the Wasserstein (\textita.k.a. earth mover’s or optimal transport)
  distance as the fitting error between each original point and its reconstruction,
  and we propose scalable algorithms to to so. Our methods build upon Fenchel duality
  and entropic regularization of Wasserstein distances, which improves not only speed
  but also computational stability. We apply these techniques on face images and text
  documents. We show in particular that we can learn dictionaries (topics) for bag-of-word
  representations of texts using words that may not have appeared in the original
  texts, or even words that come from a different language than that used in the texts.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: rolet16
month: 0
tex_title: Fast Dictionary Learning with a Smoothed Wasserstein Loss
firstpage: 630
lastpage: 638
page: 630-638
sections: 
author:
- given: Antoine
  family: Rolet
- given: Marco
  family: Cuturi
- given: Gabriel
  family: Peyré
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/rolet16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
