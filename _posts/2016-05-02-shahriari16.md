---
title: Unbounded Bayesian Optimization via Regularization
abstract: Bayesian optimization has recently emerged as a powerful and flexible tool
  in machine learning for hyperparameter tuning and more generally for the efficient
  global optimization of expensive black box functions. The established practice requires
  a user-defined bounded domain, which is assumed to contain the global optimizer.
  However, when little is known about the probed objective function, it can be difficult
  to prescribe such a domain. In this work, we modify the standard Bayesian optimization
  framework in a principled way to allow for unconstrained exploration of the search
  space. We introduce a new alternative method and compare it to a volume doubling
  baseline on two common synthetic benchmarking test functions. Finally, we apply
  our proposed methods on the task of tuning the stochastic gradient descent optimizer
  for both a multi-layered perceptron and a convolutional neural network on the MNIST
  dataset.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: shahriari16
month: 0
firstpage: 1168
lastpage: 1176
page: 1168-1176
sections: 
author:
- given: Bobak
  family: Shahriari
- given: Alexandre
  family: Bouchard-Cote
- given: Nando
  family: Freitas
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/shahriari16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
