---
supplementary: Supplementary:song16-supp.pdf
title: Learning Sigmoid Belief Networks via Monte Carlo Expectation Maximization
abstract: Belief networks are commonly used generative models of data, but require
  expensive posterior estimation to train and test the model. Learning typically proceeds
  by posterior sampling, variational approximations, or recognition networks, combined
  with stochastic optimization. We propose using an online Monte Carlo expectation-maximization
  (MCEM) algorithm to learn the maximum a posteriori (MAP) estimator of the generative
  model or optimize the variational lower bound of a recognition network. The E-step
  in this algorithm requires posterior samples, which are already generated in current
  learning schema. For the M-step, we augment with Polya-Gamma (PG) random variables
  to give an analytic updating scheme. We show relationships to standard learning
  approaches by deriving stochastic gradient ascent in the MCEM framework. We apply
  the proposed methods to both binary and count data. Experimental results show that
  MCEM improves the convergence speed and often improves hold-out performance over
  existing learning methods. Our approach is readily generalized to other recognition
  networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: song16
month: 0
firstpage: 1347
lastpage: 1355
page: 1347-1355
sections: 
author:
- given: Zhao
  family: Song
- given: Ricardo
  family: Henao
- given: David
  family: Carlson
- given: Lawrence
  family: Carin
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/song16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
