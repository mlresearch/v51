---
supplementary: Supplementary:sra16-supp.pdf
title: 'AdaDelay: Delay Adaptive Distributed Stochastic Optimization'
abstract: We develop distributed stochastic convex optimization algorithms under a
  delayed gradient model in which server nodes update parameters and worker nodes
  compute stochastic (sub)gradients. Our setup is motivated by the behavior of real-world
  distributed computation systems; in particular, we analyze a setting wherein worker
  nodes can be differently slow at different times. In contrast to existing approaches,
  we do not impose a worst-case bound on the delays experienced but rather allow the
  updates to be sensitive to the actual delays experienced. This sensitivity allows
  use of larger stepsizes, which can help speed up initial convergence without having
  to wait too long for slower machines; the global convergence rate is still preserved.
  We experiment with different delay patterns, and obtain noticeable  improvements
  for large-scale real datasets with billions of examples and features.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: sra16
month: 0
tex_title: 'AdaDelay: Delay Adaptive Distributed Stochastic Optimization'
firstpage: 957
lastpage: 965
page: 957-965
sections: 
author:
- given: Suvrit
  family: Sra
- given: Adams Wei
  family: Yu
- given: Mu
  family: Li
- given: Alex
  family: Smola
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/sra16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
