---
supplementary: Supplementary:toulis16-supp.pdf
title: Towards Stability and Optimality in Stochastic Gradient Descent
abstract: Iterative procedures for parameter estimation based on stochastic gradient
  descent (SGD) allow the estimation to scale to massive data sets. However, they
  typically suffer from numerical instability, while estimators based on SGD are statistically
  inefficient as they do not use all the information in the data set. To address these
  two issues we propose an iterative estimation procedure termed averaged implicit
  SGD (AI-SGD). For statistical efficiency AI-SGD employs averaging of the iterates,
  which achieves the Cramer-Rao bound under strong convexity, i.e., it is asymptotically
  an optimal unbiased estimator of the true parameter value. For numerical stability
  AI-SGD employs an implicit update at each iteration, which is similar to updates
  performed by proximal operators in optimization. In practice, AI-SGD achieves competitive
  performance with state-of-the-art procedures. Furthermore, it is more stable than
  averaging procedures that do not employ proximal updates, and is simple to implement
  as it requires fewer tunable hyperparameters than procedures that do employ proximal
  updates.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: toulis16
month: 0
firstpage: 1290
lastpage: 1298
page: 1290-1298
sections: 
author:
- given: Panos
  family: Toulis
- given: Dustin
  family: Tran
- given: Edo
  family: Airoldi
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/toulis16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
