---
supplementary: Supplementary:wilson16-supp.pdf
title: Deep Kernel Learning
abstract: We introduce scalable deep kernels, which combine the structural properties
  of deep learning architectures with the non-parametric flexibility of kernel methods.  Specifically,
  we transform the inputs of a spectral mixture base kernel with a deep architecture,
  using local kernel interpolation, inducing points, and structure exploiting (Kronecker
  and Toeplitz) algebra for a scalable kernel representation.  These closed-form kernels
  can be used as drop-in replacements for standard kernels, with benefits in expressive
  power and scalability.  We jointly learn the properties of these kernels through
  the marginal likelihood of a Gaussian process.  Inference and learning cost O(n)
  for n training points, and predictions cost O(1) per test point.  On a large and
  diverse collection of applications, including a dataset with 2 million examples,
  we show improved performance over scalable Gaussian processes with flexible kernel
  learning models, and stand-alone deep architectures.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: wilson16
month: 0
tex_title: Deep Kernel Learning
firstpage: 370
lastpage: 378
page: 370-378
sections: 
author:
- given: Andrew Gordon
  family: Wilson
- given: Zhiting
  family: Hu
- given: Ruslan
  family: Salakhutdinov
- given: Eric P.
  family: Xing
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/wilson16.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
