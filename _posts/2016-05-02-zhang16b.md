---
supplementary: Supplementary:zhang16b-supp.pdf
title: Global Convergence of a Grassmannian Gradient Descent Algorithm for Subspace
  Estimation
abstract: It has been observed in a variety of contexts that gradient descent methods
  have great success in solving low-rank matrix factorization problems, despite the
  relevant problem formulation being non-convex. We tackle a particular instance of
  this scenario, where we seek the d-dimensional subspace spanned by a streaming data
  matrix.  We apply the natural first order incremental gradient descent method, constraining
  the gradient method to the Grassmannian. In this paper, we propose an adaptive step
  size scheme that is greedy for the noiseless case, that maximizes the improvement
  of our metric of convergence at each data index t, and yields an expected improvement
  for the noisy case. We show that, with noise-free data, this method converges from
  any random initialization to the global minimum of the problem. For noisy data,
  we provide the expected convergence rate of the proposed algorithm per iteration.
layout: inproceedings
series: Proceedings of Machine Learning Research
id: zhang16b
month: 0
tex_title: Global Convergence of a Grassmannian Gradient Descent Algorithm for Subspace
  Estimation
firstpage: 1460
lastpage: 1468
page: 1460-1468
sections: 
author:
- given: Dejiao
  family: Zhang
- given: Laura
  family: Balzano
date: 2016-05-02
address: Cadiz, Spain
publisher: PMLR
container-title: Proceedings of the 19th International Conference on Artificial Intelligence
  and Statistics
volume: '51'
genre: inproceedings
issued:
  date-parts:
  - 2016
  - 5
  - 2
pdf: http://proceedings.mlr.press/v51/zhang16b.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
