<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Time-Varying Gaussian Process Bandit Optimization | AISTATS 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Time-Varying Gaussian Process Bandit Optimization">

  <meta name="citation_author" content="Bogunovic, Ilija">

  <meta name="citation_author" content="Scarlett, Jonathan">

  <meta name="citation_author" content="Cevher, Volkan">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of the 19th International Conference on Artificial Intelligence and Statistics">
<meta name="citation_firstpage" content="314">
<meta name="citation_lastpage" content="323">
<meta name="citation_pdf_url" content="bogunovic16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Time-Varying Gaussian Process Bandit Optimization</h1>

	<div id="authors">
	
		Ilija Bogunovic,
	
		Jonathan Scarlett,
	
		Volkan Cevher
	<br />
	</div>
	<div id="info">
		Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,
		pp. 314â€“323, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We consider the sequential Bayesian optimization problem with bandit feedback, adopting a formulation that allows for the reward function to vary with time. We model the reward function using a Gaussian process whose evolution obeys a simple Markov model. We introduce two natural extensions of the classical Gaussian process upper confidence bound (GP-UCB) algorithm. The first, R-GP-UCB, resets GP-UCB at regular intervals. The second, TV-GP-UCB, instead forgets about old data in a smooth fashion. Our main contribution comprises of novel regret bounds for these algorithms, providing an explicit characterization of the trade-off between the time horizon and the rate at which the function varies. We illustrate the performance of the algorithms on both synthetic and real data, and we find the gradual forgetting of TV-GP-UCB to perform favorably compared to the sharp resetting of R-GP-UCB. Moreover, both algorithms significantly outperform classical GP-UCB, since it treats stale and fresh data equally.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="bogunovic16.pdf">Download PDF</a></li>
			
			<li><a href="bogunovic16-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
