<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Nonparametric Budgeted Stochastic Gradient Descent | AISTATS 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Nonparametric Budgeted Stochastic Gradient Descent">

  <meta name="citation_author" content="Le, Trung">

  <meta name="citation_author" content="Nguyen, Vu">

  <meta name="citation_author" content="Nguyen, Tu Dinh">

  <meta name="citation_author" content="Phung, Dinh">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of the 19th International Conference on Artificial Intelligence and Statistics">
<meta name="citation_firstpage" content="654">
<meta name="citation_lastpage" content="572">
<meta name="citation_pdf_url" content="le16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Nonparametric Budgeted Stochastic Gradient Descent</h1>

	<div id="authors">
	
		Trung Le,
	
		Vu Nguyen,
	
		Tu Dinh Nguyen,
	
		Dinh Phung
	<br />
	</div>
	<div id="info">
		Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,
		pp. 654â€“572, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		One of the most challenging problems in kernel online learning is to bound the model size. Budgeted kernel online learning addresses this issue by bounding the model size to a predefined budget. However, determining an appropriate value for such predefined budget is arduous. In this paper, we propose the Nonparametric Budgeted Stochastic Gradient Descent that allows the model size to automatically grow with data in a principled way. We provide theoretical analysis to show that our framework is guaranteed to converge for a large collection of loss functions (e.g. Hinge, Logistic, L2, L1, and <span class="math">\(\varepsilon\)</span>-insensitive) which enables the proposed algorithm to perform both classification and regression tasks without hurting the ideal convergence rate <span class="math">\(O\left(\frac{1}{T}\right)\)</span> of the standard Stochastic Gradient Descent. We validate our algorithm on the real-world datasets to consolidate the theoretical claims.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="le16.pdf">Download PDF</a></li>
			
			<li><a href="le16-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
