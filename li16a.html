<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>A Lasso-based Sparse Knowledge Gradient Policy for Sequential Optimal Learning | AISTATS 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="A Lasso-based Sparse Knowledge Gradient Policy for Sequential Optimal Learning">

  <meta name="citation_author" content="Li, Yan">

  <meta name="citation_author" content="Liu, Han">

  <meta name="citation_author" content="Powell, Warren">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of the 19th International Conference on Artificial Intelligence and Statistics">
<meta name="citation_firstpage" content="417">
<meta name="citation_lastpage" content="425">
<meta name="citation_pdf_url" content="li16a.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>A Lasso-based Sparse Knowledge Gradient Policy for Sequential Optimal Learning</h1>

	<div id="authors">
	
		Yan Li,
	
		Han Liu,
	
		Warren Powell
	<br />
	</div>
	<div id="info">
		Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,
		pp. 417â€“425, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We propose a sequential learning policy for noisy discrete global optimization and ranking and selection (R&amp;S) problems with high dimensional sparse belief functions, where there are hundreds or even thousands of features, but only a small portion of these features contain explanatory power. Our problem setting, motivated by the experimental sciences, arises where we have to choose which experiment to run next. Here the experiments are time-consuming and expensive. We derive a sparse knowledge gradient (SpKG) decision policy based on the <span class="math">\(\ell_1\)</span>-penalized regression Lasso to identify the sparsity pattern before our budget is exhausted. This policy is a unique and novel hybrid of Bayesian R&amp;S with a frequentist learning approach. Theoretically, we provide the error bound of the posterior mean estimate, which has shown to be at the minimax optimal <span class="math">\(\sqrt{s\log p/n}\)</span> rate. Controlled experiments on both synthetic data and real application for automatically designing experiments to identify the structure of an RNA molecule show that the algorithm efficiently learns the correct set of nonzero parameters. It also outperforms several other learning policies.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="li16a.pdf">Download PDF</a></li>
			
			<li><a href="li16a-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
