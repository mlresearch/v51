<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>Accelerating Online Convex Optimization via Adaptive Prediction | AISTATS 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Accelerating Online Convex Optimization via Adaptive Prediction">

  <meta name="citation_author" content="Mohri, Mehryar">

  <meta name="citation_author" content="Yang, Scott">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of the 19th International Conference on Artificial Intelligence and Statistics">
<meta name="citation_firstpage" content="848">
<meta name="citation_lastpage" content="856">
<meta name="citation_pdf_url" content="mohri16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>Accelerating Online Convex Optimization via Adaptive Prediction</h1>

	<div id="authors">
	
		Mehryar Mohri,
	
		Scott Yang
	<br />
	</div>
	<div id="info">
		Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,
		pp. 848â€“856, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		We present a powerful general framework for designing data-dependent online convex optimization algorithms, building upon and unifying recent techniques in adaptive regularization, optimistic gradient predictions, and problem-dependent randomization. We first present a series of new regret guarantees that hold at any time and under very minimal assumptions, and then show how different relaxations recover existing algorithms, both basic as well as more recent sophisticated ones. Finally, we show how combining adaptivity, optimism, and problem-dependent randomization can guide the design of algorithms that benefit from more favorable guarantees than recent state-of-the-art methods.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="mohri16.pdf">Download PDF</a></li>
			
			<li><a href="mohri16-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
