<?xml version="1.0" encoding="UTF-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<title>On Convergence of Model Parallel Proximal Gradient Algorithm for Stale Synchronous Parallel System | AISTATS 2016 | JMLR W&amp;CP</title>

	<!-- Stylesheet -->
	<link rel="stylesheet" type="text/css" href="../css/jmlr.css" />

	<!-- Fixed position navigation -->
	<!--#include virtual="/proceedings/css-scroll.txt"-->

	<!-- MathJax -->
	<script type="text/x-mathjax-config">
	MathJax.Hub.Config({tex2jax: {inlineMath: [['\\(','\\)']]}});
</script>
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


	<!-- Metadata -->
	<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="On Convergence of Model Parallel Proximal Gradient Algorithm for Stale Synchronous Parallel System">

  <meta name="citation_author" content="Zhou, Yi">

  <meta name="citation_author" content="Yu, Yaoliang">

  <meta name="citation_author" content="Dai, Wei">

  <meta name="citation_author" content="Liang, Yingbin">

  <meta name="citation_author" content="Xing, Eric">

<meta name="citation_publication_date" content="2016">
<meta name="citation_conference_title" content="Proceedings of the 19th International Conference on Artificial Intelligence and Statistics">
<meta name="citation_firstpage" content="713">
<meta name="citation_lastpage" content="722">
<meta name="citation_pdf_url" content="zhou16.pdf">

</head>
<body>


<div id="fixed">
<!--#include virtual="/proceedings/nav-bar.txt"-->
</div>

<div id="content">

	<h1>On Convergence of Model Parallel Proximal Gradient Algorithm for Stale Synchronous Parallel System</h1>

	<div id="authors">
	
		Yi Zhou,
	
		Yaoliang Yu,
	
		Wei Dai,
	
		Yingbin Liang,
	
		Eric Xing
	<br />
	</div>
	<div id="info">
		Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,
		pp. 713â€“722, 2016
	</div> <!-- info -->

	

	<h2>Abstract</h2>
	<div id="abstract">
		With ever growing data volume and model size, an error-tolerant, communication efficient, yet versatile parallel algorithm has become a vital part for the success of many large-scale applications. In this work we propose mspg, an extension of the flexible proximal gradient algorithm to the model parallel and stale synchronous setting. The worker machines of mspg operate asynchronously as long as they are not too far apart, and they communicate efficiently through a dedicated parameter server. Theoretically, we provide a rigorous analysis of the various convergence properties of mspg, and a salient feature of our analysis is its seamless generality that allows both nonsmooth and nonconvex functions. Under mild conditions, we prove the whole iterate sequence of mspg converges to a critical point (which is optimal under convexity assumptions). We further provide an economical implementation of mspg, completely bypassing the need of keeping a local full model. We confirm our theoretical findings through numerical experiments.
	</div>

	<h2>Related Material</h2>
	<div id="extras">
		<ul>
			<li><a href="zhou16.pdf">Download PDF</a></li>
			
			<li><a href="zhou16-supp.pdf">Supplementary (PDF)</a></li>
			
			
		</ul>
	</div> <!-- extras -->

</div> <!-- content -->

</body>
</html>
